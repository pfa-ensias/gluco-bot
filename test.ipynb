{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--_yJdDL7ZHt",
        "outputId": "c0465c4b-e1bb-4060-eb58-b5b8295f54e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Specify the path to your model file in Google Drive\n",
        "model_path = '/content/drive/MyDrive/best_model_101class.hdf5'\n",
        "\n",
        "# Verify if the model file exists\n",
        "if os.path.exists(model_path):\n",
        "    print('Model file exists.')\n",
        "else:\n",
        "    print('Model file not found.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrJXc0717y7P",
        "outputId": "c6c62a57-8d7b-4aa3-9c87-8afb5ea91e67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model file exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import stat\n",
        "import seaborn as sns\n",
        "import collections\n",
        "import h5py\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.image as img\n",
        "import random\n",
        "import cv2\n",
        "import PIL\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as img\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from collections import defaultdict\n",
        "from ipywidgets import interact, interactive, fixed\n",
        "import ipywidgets as widgets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from skimage.io import imread\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "from keras.models import load_model\n",
        "from shutil import copy\n",
        "from shutil import copytree, rmtree\n",
        "import tensorflow.keras.backend \n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, AveragePooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import models\n",
        "\n",
        "%cd content/\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYQ5RE-59qPr",
        "outputId": "580c4130-4918-48ab-c7a9-44b145b19ef0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'content/'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model = load_model(model_path, compile=False)\n"
      ],
      "metadata": {
        "id": "Qg_v3k597zL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_N = {}\n",
        "N_class = {}\n",
        "with open('classes.txt', 'r') as txt:\n",
        "    classes = [i.strip() for i in txt.readlines()]\n",
        "    class_N = dict(zip(classes, range(len(classes))))\n",
        "    N_class = dict(zip(range(len(classes)), classes))\n",
        "    class_N = {i: j for j, i in N_class.items()}\n",
        "class_N_sorted = collections.OrderedDict(sorted(class_N.items()))\n",
        "print(class_N)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_cPhQo-BzAx",
        "outputId": "7ea994f8-94b3-4837-f378-6fa9f0b0d0e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'apple_pie': 0, 'baby_back_ribs': 1, 'baklava': 2, 'beef_carpaccio': 3, 'beef_tartare': 4, 'beet_salad': 5, 'beignets': 6, 'bibimbap': 7, 'bread_pudding': 8, 'breakfast_burrito': 9, 'bruschetta': 10, 'caesar_salad': 11, 'cannoli': 12, 'caprese_salad': 13, 'carrot_cake': 14, 'ceviche': 15, 'cheesecake': 16, 'cheese_plate': 17, 'chicken_curry': 18, 'chicken_quesadilla': 19, 'chicken_wings': 20, 'chocolate_cake': 21, 'chocolate_mousse': 22, 'churros': 23, 'clam_chowder': 24, 'club_sandwich': 25, 'crab_cakes': 26, 'creme_brulee': 27, 'croque_madame': 28, 'cup_cakes': 29, 'deviled_eggs': 30, 'donuts': 31, 'dumplings': 32, 'edamame': 33, 'eggs_benedict': 34, 'escargots': 35, 'falafel': 36, 'filet_mignon': 37, 'fish_and_chips': 38, 'foie_gras': 39, 'french_fries': 40, 'french_onion_soup': 41, 'french_toast': 42, 'fried_calamari': 43, 'fried_rice': 44, 'frozen_yogurt': 45, 'garlic_bread': 46, 'gnocchi': 47, 'greek_salad': 48, 'grilled_cheese_sandwich': 49, 'grilled_salmon': 50, 'guacamole': 51, 'gyoza': 52, 'hamburger': 53, 'hot_and_sour_soup': 54, 'hot_dog': 55, 'huevos_rancheros': 56, 'hummus': 57, 'ice_cream': 58, 'lasagna': 59, 'lobster_bisque': 60, 'lobster_roll_sandwich': 61, 'macaroni_and_cheese': 62, 'macarons': 63, 'miso_soup': 64, 'mussels': 65, 'nachos': 66, 'omelette': 67, 'onion_rings': 68, 'oysters': 69, 'pad_thai': 70, 'paella': 71, 'pancakes': 72, 'panna_cotta': 73, 'peking_duck': 74, 'pho': 75, 'pizza': 76, 'pork_chop': 77, 'poutine': 78, 'prime_rib': 79, 'pulled_pork_sandwich': 80, 'ramen': 81, 'ravioli': 82, 'red_velvet_cake': 83, 'risotto': 84, 'samosa': 85, 'sashimi': 86, 'scallops': 87, 'seaweed_salad': 88, 'shrimp_and_grits': 89, 'spaghetti_bolognese': 90, 'spaghetti_carbonara': 91, 'spring_rolls': 92, 'steak': 93, 'strawberry_shortcake': 94, 'sushi': 95, 'tacos': 96, 'takoyaki': 97, 'tiramisu': 98, 'tuna_tartare': 99, 'waffles': 100}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6ryYqCVEB2GJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get model layers\n",
        "layers = [layer.output for layer in model.layers[1:11]]\n",
        "layer_names = []\n",
        "\n",
        "activations_output = models.Model(inputs=model.input, outputs=layers)\n",
        "\n",
        "for layer in model.layers[1:11]:\n",
        "    layer_names.append(layer.name)\n",
        "\n",
        "print(layer_names)\n",
        "print(len(model.layers))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cg58p4h9-AM_",
        "outputId": "0e1d6b86-8b05-4e7f-c850-3b455ca51ea8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['conv2d', 'batch_normalization', 'activation', 'conv2d_1', 'batch_normalization_1', 'activation_1', 'conv2d_2', 'batch_normalization_2', 'activation_2', 'max_pooling2d']\n",
            "315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load and preprocess the image for prediction\n",
        "def preprocess_image(image_path):\n",
        "    img = cv2.imread(image_path)\n",
        "    img = cv2.resize(img, (200, 200))  # Resize the image to match the input size of the model\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
        "    img = img.astype(np.float32) / 255.0  # Normalize the image\n",
        "    img = np.expand_dims(img, axis=0)  # Add an extra dimension to represent the batch\n",
        "    return img\n",
        "\n",
        "# Make predictions on new images\n",
        "def predict(image_path):\n",
        "    # Preprocess the image\n",
        "    img = preprocess_image(image_path)\n",
        "    \n",
        "    # Make predictions\n",
        "    predictions = model.predict(img)\n",
        "    \n",
        "    # Get the predicted class label\n",
        "    class_index = np.argmax(predictions)\n",
        "    class_label = N_class[class_index]  # Assuming you have the N_class dictionary\n",
        "    \n",
        "    return class_label\n",
        "\n",
        "\n",
        "\n",
        "# Make prediction\n",
        "predicted_class = predict('teest.jpeg')\n",
        "print('Predicted Class:', predicted_class)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGG9Q0_TB_BC",
        "outputId": "c6e474cc-f553-4f07-a416-a5e083a4c939"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 113ms/step\n",
            "Predicted Class: garlic_bread\n"
          ]
        }
      ]
    }
  ]
}